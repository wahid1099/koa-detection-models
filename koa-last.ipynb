{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12677032,"sourceType":"datasetVersion","datasetId":8011156}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==========================================================\n# Hybrid CNN-ViT Architecture for Knee Osteoarthritis\n# Stage 1: EfficientNet (Local Features)\n# Stage 2: Vision Transformer (Global Context)\n# Stage 3: Attention Fusion Module\n# Stage 4: Hierarchical Classification (Binaryâ†’Ternaryâ†’5-Class)\n# ==========================================================\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.optimizers import AdamW\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (confusion_matrix, classification_report, \n                             roc_auc_score, roc_curve, auc, \n                             precision_recall_fscore_support)\nfrom sklearn.preprocessing import label_binarize\nfrom scipy import signal\nfrom scipy.ndimage import gaussian_filter\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ---------------------------\n# Configuration\n# ---------------------------\nCONFIG = {\n    'WORK_DIR': './',\n    'EPOCHS': 100,\n    'BATCH_SIZE': 8,\n    'IMG_SIZE': 224,\n    'NUM_CLASSES': 5,\n    'LEARNING_RATE': 1e-4,\n    'WEIGHT_DECAY': 1e-5,\n    'PATCH_SIZE': 16,\n    'NUM_PATCHES': (224 // 16) ** 2,  # 196 patches\n    'PROJECTION_DIM': 512,\n    'NUM_HEADS': 12,\n    'TRANSFORMER_LAYERS': 4,\n    'MLP_HEAD_UNITS': [2048, 1024],\n}\n\nPATHS = {\n    'train': '/kaggle/input/koa-dataset/dataset/train',\n    'val': '/kaggle/input/koa-dataset/dataset/val',\n    'test': '/kaggle/input/koa-dataset/dataset/test'\n}\n\nCLASS_NAMES = ['KL-0', 'KL-1', 'KL-2', 'KL-3', 'KL-4']\n\n# ---------------------------\n# 1. ADVANCED PREPROCESSING\n# ---------------------------\n\nclass KalmanFilter1D:\n    \"\"\"1D Kalman filter for noise reduction\"\"\"\n    def __init__(self, process_variance=1e-5, measurement_variance=1e-1):\n        self.process_variance = process_variance\n        self.measurement_variance = measurement_variance\n        self.posteri_estimate = 0.0\n        self.posteri_error_estimate = 1.0\n    \n    def update(self, measurement):\n        # Prediction\n        priori_estimate = self.posteri_estimate\n        priori_error_estimate = self.posteri_error_estimate + self.process_variance\n        \n        # Update\n        blending_factor = priori_error_estimate / (priori_error_estimate + self.measurement_variance)\n        self.posteri_estimate = priori_estimate + blending_factor * (measurement - priori_estimate)\n        self.posteri_error_estimate = (1 - blending_factor) * priori_error_estimate\n        \n        return self.posteri_estimate\n\ndef apply_kalman_filter(image):\n    \"\"\"Apply Kalman filter to each row/column for noise reduction\"\"\"\n    filtered = np.zeros_like(image, dtype=np.float32)\n    \n    # Filter rows\n    for i in range(image.shape[0]):\n        kf = KalmanFilter1D()\n        for j in range(image.shape[1]):\n            filtered[i, j] = kf.update(image[i, j])\n    \n    # Filter columns\n    for j in range(image.shape[1]):\n        kf = KalmanFilter1D()\n        for i in range(image.shape[0]):\n            filtered[i, j] = kf.update(filtered[i, j])\n    \n    return filtered\n\ndef laplacian_sharpen(image, kernel_size=3, strength=1.5):\n    \"\"\"Laplacian sharpening for edge enhancement\"\"\"\n    # Ensure image is in correct format\n    img_float = image.astype(np.float32)\n    \n    laplacian_kernel = np.array([[-1, -1, -1],\n                                  [-1,  9, -1],\n                                  [-1, -1, -1]], dtype=np.float32)\n    \n    sharpened = cv2.filter2D(img_float, -1, laplacian_kernel)\n    result = img_float + (sharpened - img_float) * (strength - 1.0)\n    \n    return np.clip(result, 0, 255).astype(np.uint8)\n\ndef high_pass_filter(image, kernel_size=15):\n    \"\"\"High-pass filter for texture emphasis\"\"\"\n    # Ensure image is float32 for processing\n    img_float = image.astype(np.float32)\n    \n    # Apply Gaussian blur (low-pass)\n    low_pass = cv2.GaussianBlur(img_float, (kernel_size, kernel_size), 0)\n    \n    # Subtract from original to get high-pass\n    high_pass = img_float - low_pass\n    \n    # Add back to original for enhancement\n    enhanced = img_float + (high_pass * 0.5)\n    \n    return np.clip(enhanced, 0, 255).astype(np.uint8)\n\ndef detect_knee_roi(image):\n    \"\"\"\n    Detect knee joint ROI using edge detection and morphology\n    (Simplified version - in production, use YOLOv8)\n    \"\"\"\n    # Convert to grayscale if needed\n    if len(image.shape) == 3:\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    else:\n        gray = image.copy()\n    \n    # Apply CLAHE for better contrast\n    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n    enhanced = clahe.apply(gray)\n    \n    # Edge detection\n    edges = cv2.Canny(enhanced, 50, 150)\n    \n    # Morphological operations to find joint region\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n    closed = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)\n    dilated = cv2.dilate(closed, kernel, iterations=2)\n    \n    # Find contours\n    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    if contours:\n        # Get largest contour (assumed to be knee joint)\n        largest_contour = max(contours, key=cv2.contourArea)\n        x, y, w, h = cv2.boundingRect(largest_contour)\n        \n        # Add padding\n        padding = 20\n        x = max(0, x - padding)\n        y = max(0, y - padding)\n        w = min(image.shape[1] - x, w + 2 * padding)\n        h = min(image.shape[0] - y, h + 2 * padding)\n        \n        return (x, y, w, h)\n    \n    # If no contour found, return center crop\n    h, w = image.shape[:2]\n    crop_size = min(h, w) // 2\n    x = (w - crop_size) // 2\n    y = (h - crop_size) // 2\n    return (x, y, crop_size, crop_size)\n\ndef advanced_preprocess(img_path, target_size=224):\n    \"\"\"\n    Complete preprocessing pipeline:\n    1. Load and initial CLAHE\n    2. Laplacian sharpening\n    3. High-pass filtering\n    4. Kalman filter noise reduction\n    5. ROI extraction\n    6. Resize and normalize\n    \"\"\"\n    # Load image\n    img = cv2.imread(img_path)\n    if img is None:\n        return None\n    \n    # Convert to grayscale\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    # Step 1: Initial CLAHE\n    clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))\n    enhanced = clahe.apply(gray)\n    \n    # Step 2: Laplacian sharpening\n    sharpened = laplacian_sharpen(enhanced, strength=1.3)\n    \n    # Step 3: High-pass filtering\n    high_pass = high_pass_filter(sharpened, kernel_size=15)\n    \n    # Step 4: Kalman filter (applied on normalized image)\n    normalized = high_pass.astype(np.float32) / 255.0\n    filtered = apply_kalman_filter(normalized)\n    filtered = (filtered * 255).astype(np.uint8)\n    \n    # Step 5: ROI extraction\n    x, y, w, h = detect_knee_roi(filtered)\n    roi = filtered[y:y+h, x:x+w]\n    \n    # Step 6: Resize to target size\n    resized = cv2.resize(roi, (target_size, target_size), interpolation=cv2.INTER_LANCZOS4)\n    \n    # Convert to RGB and normalize to [-1, 1]\n    rgb = cv2.cvtColor(resized, cv2.COLOR_GRAY2RGB)\n    final = (rgb.astype(np.float32) / 127.5) - 1.0\n    \n    return final, (x, y, w, h)\n\n# ---------------------------\n# 2. DATA LOADING\n# ---------------------------\n\ndef build_dataset(data_dir, target_size=224):\n    \"\"\"Build dataset with advanced preprocessing\"\"\"\n    filepaths, labels, rois = [], [], []\n    \n    for klass in sorted(os.listdir(data_dir)):\n        klass_path = os.path.join(data_dir, klass)\n        if not os.path.isdir(klass_path):\n            continue\n        \n        klass_idx = int(klass)\n        \n        for fname in os.listdir(klass_path):\n            img_path = os.path.join(klass_path, fname)\n            \n            # Preprocess\n            result = advanced_preprocess(img_path, target_size)\n            if result is not None:\n                preprocessed_img, roi_coords = result\n                filepaths.append(preprocessed_img)\n                labels.append(klass_idx)\n                rois.append(roi_coords)\n    \n    return np.array(filepaths), np.array(labels), rois\n\n# ---------------------------\n# 3. VISION TRANSFORMER COMPONENTS\n# ---------------------------\n\nclass PatchExtractor(layers.Layer):\n    \"\"\"Extract patches from images\"\"\"\n    def __init__(self, patch_size):\n        super().__init__()\n        self.patch_size = patch_size\n    \n    def call(self, images):\n        batch_size = tf.shape(images)[0]\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=[1, self.patch_size, self.patch_size, 1],\n            strides=[1, self.patch_size, self.patch_size, 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\",\n        )\n        patch_dims = patches.shape[-1]\n        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n        return patches\n\nclass PatchEncoder(layers.Layer):\n    \"\"\"Encode patches with position embeddings\"\"\"\n    def __init__(self, num_patches, projection_dim):\n        super().__init__()\n        self.num_patches = num_patches\n        self.projection = layers.Dense(units=projection_dim)\n        self.position_embedding = layers.Embedding(\n            input_dim=num_patches, output_dim=projection_dim\n        )\n    \n    def call(self, patch):\n        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n        encoded = self.projection(patch) + self.position_embedding(positions)\n        return encoded\n\nclass TransformerBlock(layers.Layer):\n    \"\"\"Transformer encoder block\"\"\"\n    def __init__(self, projection_dim, num_heads, mlp_dim, dropout=0.1):\n        super().__init__()\n        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.attn = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=projection_dim, dropout=dropout\n        )\n        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.mlp = keras.Sequential([\n            layers.Dense(mlp_dim, activation=tf.nn.gelu),\n            layers.Dropout(dropout),\n            layers.Dense(projection_dim),\n            layers.Dropout(dropout),\n        ])\n    \n    def call(self, encoded_patches, training):\n        # Multi-head attention\n        x1 = self.norm1(encoded_patches)\n        attention_output = self.attn(x1, x1, training=training)\n        x2 = layers.Add()([attention_output, encoded_patches])\n        \n        # MLP\n        x3 = self.norm2(x2)\n        x3 = self.mlp(x3, training=training)\n        encoded_patches = layers.Add()([x3, x2])\n        \n        return encoded_patches\n\n# ---------------------------\n# 4. CNN FEATURE EXTRACTOR\n# ---------------------------\n\ndef build_cnn_backbone(input_shape=(224, 224, 3)):\n    \"\"\"EfficientNetB0 for local feature extraction\"\"\"\n    inputs = layers.Input(shape=input_shape)\n    \n    # EfficientNetB0 backbone\n    backbone = EfficientNetB0(\n        include_top=False,\n        weights='imagenet',\n        input_tensor=inputs\n    )\n    \n    # Get features from multiple scales\n    # We'll use the output before global pooling\n    features = backbone.output  # Shape: (None, 7, 7, 1792)\n    \n    return Model(inputs, features, name='CNN_Backbone')\n\n# ---------------------------\n# 5. ATTENTION FUSION MODULE\n# ---------------------------\n\nclass AttentionFusion(layers.Layer):\n    \"\"\"Fuse CNN and ViT features with learned attention\"\"\"\n    def __init__(self, fusion_dim=1024):\n        super().__init__()\n        self.fusion_dim = fusion_dim\n        \n        # Projection layers\n        self.cnn_proj = layers.Dense(fusion_dim, activation='relu')\n        self.vit_proj = layers.Dense(fusion_dim, activation='relu')\n        \n        # Attention weights\n        self.cnn_attention = layers.Dense(1, activation='sigmoid')\n        self.vit_attention = layers.Dense(1, activation='sigmoid')\n        \n        # Final fusion\n        self.fusion_layer = layers.Dense(fusion_dim, activation='relu')\n        self.dropout = layers.Dropout(0.3)\n    \n    def call(self, cnn_features, vit_features, training=False):\n        # Project to same dimension\n        cnn_proj = self.cnn_proj(cnn_features)\n        vit_proj = self.vit_proj(vit_features)\n        \n        # Compute attention weights\n        cnn_weight = self.cnn_attention(cnn_proj)\n        vit_weight = self.vit_attention(vit_proj)\n        \n        # Normalize weights\n        total_weight = cnn_weight + vit_weight\n        cnn_weight = cnn_weight / (total_weight + 1e-8)\n        vit_weight = vit_weight / (total_weight + 1e-8)\n        \n        # Weighted fusion\n        fused = cnn_weight * cnn_proj + vit_weight * vit_proj\n        fused = self.fusion_layer(fused)\n        fused = self.dropout(fused, training=training)\n        \n        return fused, cnn_weight, vit_weight\n\n# ---------------------------\n# 6. HIERARCHICAL CLASSIFIER\n# ---------------------------\n\nclass HierarchicalClassifier(layers.Layer):\n    \"\"\"\n    Hierarchical classification:\n    Level 1: Binary (Healthy vs OA)\n    Level 2: Ternary (Mild vs Moderate vs Severe)\n    Level 3: Fine-grained (5-class KL grade)\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        \n        # Level 1: Binary classifier (0 vs 1-4)\n        self.binary_head = keras.Sequential([\n            layers.Dense(256, activation='relu'),\n            layers.Dropout(0.3),\n            layers.Dense(2, activation='softmax', name='binary_output')\n        ])\n        \n        # Level 2: Ternary classifier (0-1 vs 2-3 vs 4)\n        self.ternary_head = keras.Sequential([\n            layers.Dense(256, activation='relu'),\n            layers.Dropout(0.3),\n            layers.Dense(3, activation='softmax', name='ternary_output')\n        ])\n        \n        # Level 3: Fine-grained classifier (0-4)\n        self.fine_head = keras.Sequential([\n            layers.Dense(512, activation='relu'),\n            layers.Dropout(0.3),\n            layers.Dense(256, activation='relu'),\n            layers.Dropout(0.3),\n            layers.Dense(5, activation='softmax', name='fine_output')\n        ])\n    \n    def call(self, features, training=False):\n        binary_pred = self.binary_head(features, training=training)\n        ternary_pred = self.ternary_head(features, training=training)\n        fine_pred = self.fine_head(features, training=training)\n        \n        return binary_pred, ternary_pred, fine_pred\n\n# ---------------------------\n# 7. COMPLETE HYBRID MODEL\n# ---------------------------\n\ndef build_hybrid_model(config=CONFIG):\n    \"\"\"\n    Build complete hybrid CNN-ViT model\n    \"\"\"\n    img_size = config['IMG_SIZE']\n    patch_size = config['PATCH_SIZE']\n    num_patches = config['NUM_PATCHES']\n    projection_dim = config['PROJECTION_DIM']\n    num_heads = config['NUM_HEADS']\n    transformer_layers = config['TRANSFORMER_LAYERS']\n    \n    inputs = layers.Input(shape=(img_size, img_size, 3))\n    \n    # ========== Stage 1: CNN Feature Extraction ==========\n    cnn_backbone = build_cnn_backbone((img_size, img_size, 3))\n    cnn_features = cnn_backbone(inputs)\n    \n    # Global average pooling for CNN features\n    cnn_gap = layers.GlobalAveragePooling2D()(cnn_features)\n    \n    # ========== Stage 2: Vision Transformer ==========\n    # Extract patches\n    patches = PatchExtractor(patch_size)(inputs)\n    \n    # Encode patches\n    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n    \n    # Transformer blocks\n    for _ in range(transformer_layers):\n        encoded_patches = TransformerBlock(\n            projection_dim=projection_dim,\n            num_heads=num_heads,\n            mlp_dim=projection_dim * 4,\n            dropout=0.1\n        )(encoded_patches, training=True)\n    \n    # Global representation from ViT\n    vit_representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n    vit_gap = layers.GlobalAveragePooling1D()(vit_representation)\n    \n    # ========== Stage 3: Attention Fusion ==========\n    fusion_module = AttentionFusion(fusion_dim=1024)\n    fused_features, cnn_weight, vit_weight = fusion_module(cnn_gap, vit_gap)\n    \n    # ========== Stage 4: Hierarchical Classification ==========\n    hierarchical_classifier = HierarchicalClassifier()\n    binary_output, ternary_output, fine_output = hierarchical_classifier(fused_features)\n    \n    # Build model\n    model = Model(\n        inputs=inputs,\n        outputs={\n            'binary': binary_output,\n            'ternary': ternary_output,\n            'fine': fine_output\n        },\n        name='Hybrid_CNN_ViT_KOA'\n    )\n    \n    return model\n\n# ---------------------------\n# 8. CUSTOM LOSS FUNCTIONS\n# ---------------------------\n\ndef hierarchical_loss(y_true_fine, y_pred_binary, y_pred_ternary, y_pred_fine, \n                      alpha=0.3, beta=0.3, gamma=0.4):\n    \"\"\"\n    Combined hierarchical loss\n    \"\"\"\n    # Convert fine labels to hierarchical labels\n    # Binary: 0 -> class 0, 1-4 -> class 1\n    y_true_binary = tf.cast(y_true_fine > 0, tf.int32)\n    y_true_binary = tf.one_hot(y_true_binary, 2)\n    \n    # Ternary: 0-1 -> class 0, 2-3 -> class 1, 4 -> class 2\n    y_true_ternary = tf.where(y_true_fine <= 1, 0,\n                              tf.where(y_true_fine <= 3, 1, 2))\n    y_true_ternary = tf.one_hot(y_true_ternary, 3)\n    \n    # Fine: 0-4 -> classes 0-4\n    y_true_fine_onehot = tf.one_hot(y_true_fine, 5)\n    \n    # Compute losses\n    loss_binary = keras.losses.categorical_crossentropy(y_true_binary, y_pred_binary)\n    loss_ternary = keras.losses.categorical_crossentropy(y_true_ternary, y_pred_ternary)\n    loss_fine = keras.losses.categorical_crossentropy(y_true_fine_onehot, y_pred_fine)\n    \n    # Combined loss\n    total_loss = alpha * loss_binary + beta * loss_ternary + gamma * loss_fine\n    \n    return total_loss\n\n# ---------------------------\n# 9. TRAINING\n# ---------------------------\n\ndef train_model(model, train_data, val_data, config=CONFIG):\n    \"\"\"Train the hybrid model\"\"\"\n    \n    X_train, y_train = train_data\n    X_val, y_val = val_data\n    \n    # Custom training loop for hierarchical loss\n    optimizer = AdamW(learning_rate=config['LEARNING_RATE'], \n                      weight_decay=config['WEIGHT_DECAY'])\n    \n    train_loss_tracker = keras.metrics.Mean(name='train_loss')\n    val_loss_tracker = keras.metrics.Mean(name='val_loss')\n    train_acc_tracker = keras.metrics.CategoricalAccuracy(name='train_acc')\n    val_acc_tracker = keras.metrics.CategoricalAccuracy(name='val_acc')\n    \n    @tf.function\n    def train_step(x, y):\n        with tf.GradientTape() as tape:\n            predictions = model(x, training=True)\n            loss = hierarchical_loss(\n                y, \n                predictions['binary'],\n                predictions['ternary'],\n                predictions['fine']\n            )\n        \n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        \n        train_loss_tracker.update_state(loss)\n        train_acc_tracker.update_state(tf.one_hot(y, 5), predictions['fine'])\n        \n        return loss\n    \n    @tf.function\n    def val_step(x, y):\n        predictions = model(x, training=False)\n        loss = hierarchical_loss(\n            y,\n            predictions['binary'],\n            predictions['ternary'],\n            predictions['fine']\n        )\n        \n        val_loss_tracker.update_state(loss)\n        val_acc_tracker.update_state(tf.one_hot(y, 5), predictions['fine'])\n        \n        return loss\n    \n    # Training loop\n    best_val_loss = float('inf')\n    patience = 15\n    patience_counter = 0\n    \n    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n    \n    for epoch in range(config['EPOCHS']):\n        print(f\"\\nEpoch {epoch + 1}/{config['EPOCHS']}\")\n        \n        # Reset metrics (use reset_state, not reset_states)\n        train_loss_tracker.reset_state()\n        val_loss_tracker.reset_state()\n        train_acc_tracker.reset_state()\n        val_acc_tracker.reset_state()\n        \n        # Training\n        num_batches = len(X_train) // config['BATCH_SIZE']\n        for batch in range(num_batches):\n            start_idx = batch * config['BATCH_SIZE']\n            end_idx = start_idx + config['BATCH_SIZE']\n            \n            x_batch = X_train[start_idx:end_idx]\n            y_batch = y_train[start_idx:end_idx]\n            \n            train_step(x_batch, y_batch)\n            \n            if batch % 10 == 0:\n                print(f\"Batch {batch}/{num_batches} - \"\n                      f\"Loss: {train_loss_tracker.result():.4f} - \"\n                      f\"Acc: {train_acc_tracker.result():.4f}\", end='\\r')\n        \n        # Validation\n        num_val_batches = len(X_val) // config['BATCH_SIZE']\n        for batch in range(num_val_batches):\n            start_idx = batch * config['BATCH_SIZE']\n            end_idx = start_idx + config['BATCH_SIZE']\n            \n            x_batch = X_val[start_idx:end_idx]\n            y_batch = y_val[start_idx:end_idx]\n            \n            val_step(x_batch, y_batch)\n        \n        # Log results\n        train_loss = train_loss_tracker.result()\n        val_loss = val_loss_tracker.result()\n        train_acc = train_acc_tracker.result()\n        val_acc = val_acc_tracker.result()\n        \n        history['train_loss'].append(float(train_loss))\n        history['val_loss'].append(float(val_loss))\n        history['train_acc'].append(float(train_acc))\n        history['val_acc'].append(float(val_acc))\n        \n        print(f\"\\nTrain Loss: {train_loss:.4f} - Train Acc: {train_acc:.4f}\")\n        print(f\"Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.4f}\")\n        \n        # Early stopping\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            model.save_weights('best_hybrid_model.weights.h5')\n            print(\"âœ“ Model saved!\")\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n                break\n    \n    return model, history\n\n# ---------------------------\n# 10. COMPREHENSIVE EVALUATION\n# ---------------------------\n\ndef evaluate_comprehensive(model, X_test, y_test):\n    \"\"\"\n    Comprehensive evaluation with multiple metrics\n    \"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"COMPREHENSIVE MODEL EVALUATION\")\n    print(\"=\"*70)\n    \n    # Get predictions\n    predictions = model.predict(X_test, batch_size=16, verbose=1)\n    \n    # Extract fine-grained predictions\n    y_pred_probs = predictions['fine']\n    y_pred = np.argmax(y_pred_probs, axis=1)\n    \n    # Binary predictions\n    y_pred_binary = np.argmax(predictions['binary'], axis=1)\n    y_true_binary = (y_test > 0).astype(int)\n    \n    # Ternary predictions\n    y_pred_ternary = np.argmax(predictions['ternary'], axis=1)\n    y_true_ternary = np.where(y_test <= 1, 0, np.where(y_test <= 3, 1, 2))\n    \n    # ========== Binary Classification Metrics ==========\n    print(\"\\nğŸ“Š BINARY CLASSIFICATION (Healthy vs OA):\")\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        y_true_binary, y_pred_binary, average='binary'\n    )\n    print(f\"  Precision: {precision:.4f}\")\n    print(f\"  Recall:    {recall:.4f}\")\n    print(f\"  F1-Score:  {f1:.4f}\")\n    \n    # ========== Ternary Classification Metrics ==========\n    print(\"\\nğŸ“Š TERNARY CLASSIFICATION (Mild vs Moderate vs Severe):\")\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        y_true_ternary, y_pred_ternary, average='weighted'\n    )\n    print(f\"  Precision: {precision:.4f}\")\n    print(f\"  Recall:    {recall:.4f}\")\n    print(f\"  F1-Score:  {f1:.4f}\")\n    \n    # ========== Fine-grained Metrics ==========\n    print(\"\\nğŸ“Š FINE-GRAINED CLASSIFICATION (5-class KL Grade):\")\n    print(\"\\nPer-Class Metrics:\")\n    precision, recall, f1, support = precision_recall_fscore_support(\n        y_test, y_pred, average=None\n    )\n    \n    for i, class_name in enumerate(CLASS_NAMES):\n        print(f\"\\n  {class_name}:\")\n        print(f\"    Precision: {precision[i]:.4f}\")\n        print(f\"    Recall:    {recall[i]:.4f}\")\n        print(f\"    F1-Score:  {f1[i]:.4f}\")\n        print(f\"    Support:   {support[i]}\")\n    \n    # Overall metrics\n    precision_avg, recall_avg, f1_avg, _ = precision_recall_fscore_support(\n        y_test, y_pred, average='weighted'\n    )\n    print(f\"\\n  Overall (Weighted):\")\n    print(f\"    Precision: {precision_avg:.4f}\")\n    print(f\"    Recall:    {recall_avg:.4f}\")\n    print(f\"    F1-Score:  {f1_avg:.4f}\")\n    \n    # ========== Confusion Matrix ==========\n    cm = confusion_matrix(y_test, y_pred)\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n    plt.title('Confusion Matrix - Fine-grained Classification')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.tight_layout()\n    plt.savefig('confusion_matrix_hybrid.png', dpi=150)\n    plt.show()\n    \n    # ========== ROC-AUC Curves ==========\n    print(\"\\nğŸ“ˆ Computing ROC-AUC Curves...\")\n    \n    # Binarize labels for multi-class ROC\n    y_test_bin = label_binarize(y_test, classes=range(5))\n    \n    # Compute ROC curve and AUC for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    \n    for i in range(5):\n        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_probs[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n    \n    # Plot ROC curves\n    plt.figure(figsize=(12, 8))\n    colors = ['blue', 'green', 'orange', 'red', 'purple']\n    \n    for i, color in enumerate(colors):\n        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n                label=f'{CLASS_NAMES[i]} (AUC = {roc_auc[i]:.3f})')\n    \n    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate', fontsize=12)\n    plt.ylabel('True Positive Rate', fontsize=12)\n    plt.title('ROC Curves - Multi-class Classification', fontsize=14, fontweight='bold')\n    plt.legend(loc=\"lower right\")\n    plt.grid(alpha=0.3)\n    plt.tight_layout()\n    plt.savefig('roc_curves_hybrid.png', dpi=150)\n    plt.show()\n    \n    print(\"\\nâœ… Evaluation Complete!\")\n    \n    return {\n        'y_pred': y_pred,\n        'y_pred_probs': y_pred_probs,\n        'y_pred_binary': y_pred_binary,\n        'y_pred_ternary': y_pred_ternary,\n        'roc_auc': roc_auc\n    }\n\n# ---------------------------\n# 11. ATTENTION VISUALIZATION\n# ---------------------------\n\ndef visualize_attention_maps(model, X_samples, y_samples, num_samples=5):\n    \"\"\"\n    Visualize attention heatmaps for interpretability\n    \"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"ATTENTION VISUALIZATION\")\n    print(\"=\"*70)\n    \n    # Create a model that outputs intermediate attention weights\n    # We'll visualize both CNN and ViT attention\n    \n    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n    \n    for idx in range(min(num_samples, len(X_samples))):\n        img = X_samples[idx:idx+1]\n        true_label = y_samples[idx]\n        \n        # Get predictions\n        predictions = model.predict(img, verbose=0)\n        pred_label = np.argmax(predictions['fine'])\n        \n        # Original image\n        original = ((img[0] + 1.0) * 127.5).astype(np.uint8)\n        \n        # For visualization, we'll create a simple gradient-based attention map\n        with tf.GradientTape() as tape:\n            tape.watch(img)\n            predictions = model(img, training=False)\n            pred_score = predictions['fine'][0, pred_label]\n        \n        # Get gradients\n        grads = tape.gradient(pred_score, img)\n        \n        # Create attention map\n        grads_abs = tf.abs(grads)\n        attention_map = tf.reduce_mean(grads_abs, axis=-1)[0].numpy()\n        \n        # Normalize\n        attention_map = (attention_map - attention_map.min()) / (attention_map.max() - attention_map.min() + 1e-8)\n        \n        # Apply colormap\n        attention_colored = cv2.applyColorMap(\n            (attention_map * 255).astype(np.uint8),\n            cv2.COLORMAP_JET\n        )\n        \n        # Overlay\n        overlay = cv2.addWeighted(original, 0.6, attention_colored, 0.4, 0)\n        \n        # Plot\n        axes[idx, 0].imshow(original)\n        axes[idx, 0].set_title(f'Original\\nTrue: {CLASS_NAMES[true_label]}')\n        axes[idx, 0].axis('off')\n        \n        axes[idx, 1].imshow(attention_map, cmap='jet')\n        axes[idx, 1].set_title('Attention Map')\n        axes[idx, 1].axis('off')\n        \n        axes[idx, 2].imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n        axes[idx, 2].set_title(f'Overlay\\nPred: {CLASS_NAMES[pred_label]}')\n        axes[idx, 2].axis('off')\n        \n        # Show prediction probabilities\n        probs = predictions['fine'][0].numpy()\n        axes[idx, 3].barh(CLASS_NAMES, probs, color='steelblue')\n        axes[idx, 3].set_xlim([0, 1])\n        axes[idx, 3].set_title('Prediction Probabilities')\n        axes[idx, 3].set_xlabel('Probability')\n    \n    plt.tight_layout()\n    plt.savefig('attention_visualization.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    print(\"âœ… Attention maps generated!\")\n\n# ---------------------------\n# 12. MAIN EXECUTION\n# ---------------------------\n\ndef main():\n    \"\"\"\n    Complete pipeline execution\n    \"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"HYBRID CNN-ViT FOR KNEE OSTEOARTHRITIS CLASSIFICATION\")\n    print(\"=\"*70)\n    \n    # ========== STEP 1: Load and Preprocess Data ==========\n    print(\"\\n[STEP 1/5] Loading and preprocessing data...\")\n    print(\"âš ï¸  This may take several minutes due to advanced preprocessing...\")\n    \n    print(\"\\nProcessing training data...\")\n    X_train, y_train, train_rois = build_dataset(PATHS['train'], CONFIG['IMG_SIZE'])\n    \n    print(\"Processing validation data...\")\n    X_val, y_val, val_rois = build_dataset(PATHS['val'], CONFIG['IMG_SIZE'])\n    \n    print(\"Processing test data...\")\n    X_test, y_test, test_rois = build_dataset(PATHS['test'], CONFIG['IMG_SIZE'])\n    \n    print(f\"\\nâœ“ Dataset loaded:\")\n    print(f\"  Train: {len(X_train)} samples\")\n    print(f\"  Val:   {len(X_val)} samples\")\n    print(f\"  Test:  {len(X_test)} samples\")\n    \n    # ========== STEP 2: Build Model ==========\n    print(\"\\n[STEP 2/5] Building hybrid CNN-ViT model...\")\n    \n    model = build_hybrid_model(CONFIG)\n    \n    print(f\"\\nâœ“ Model built successfully!\")\n    print(f\"  Total parameters: {model.count_params():,}\")\n    \n    # Model summary\n    model.summary()\n    \n    # ========== STEP 3: Train Model ==========\n    print(\"\\n[STEP 3/5] Training model...\")\n    \n    trained_model, history = train_model(\n        model,\n        train_data=(X_train, y_train),\n        val_data=(X_val, y_val),\n        config=CONFIG\n    )\n    \n    # Load best weights\n    trained_model.load_weights('best_hybrid_model.h5')\n    print(\"\\nâœ“ Best model weights loaded!\")\n    \n    # ========== STEP 4: Evaluate Model ==========\n    print(\"\\n[STEP 4/5] Evaluating model on test set...\")\n    \n    eval_results = evaluate_comprehensive(trained_model, X_test, y_test)\n    \n    # ========== STEP 5: Visualize Attention ==========\n    print(\"\\n[STEP 5/5] Generating attention visualizations...\")\n    \n    # Select diverse samples for visualization\n    sample_indices = []\n    for class_idx in range(5):\n        class_indices = np.where(y_test == class_idx)[0]\n        if len(class_indices) > 0:\n            sample_indices.append(class_indices[0])\n    \n    if len(sample_indices) >= 5:\n        X_samples = X_test[sample_indices[:5]]\n        y_samples = y_test[sample_indices[:5]]\n        \n        visualize_attention_maps(trained_model, X_samples, y_samples, num_samples=5)\n    \n    # ========== Training History Visualization ==========\n    print(\"\\nğŸ“Š Plotting training history...\")\n    \n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Loss\n    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n    axes[0].plot(history['val_loss'], label='Validation Loss', linewidth=2)\n    axes[0].set_xlabel('Epoch', fontsize=12)\n    axes[0].set_ylabel('Loss', fontsize=12)\n    axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n    axes[0].legend()\n    axes[0].grid(alpha=0.3)\n    \n    # Accuracy\n    axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n    axes[1].plot(history['val_acc'], label='Validation Accuracy', linewidth=2)\n    axes[1].set_xlabel('Epoch', fontsize=12)\n    axes[1].set_ylabel('Accuracy', fontsize=12)\n    axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n    axes[1].legend()\n    axes[1].grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig('training_history.png', dpi=150)\n    plt.show()\n    \n    # ========== Summary ==========\n    print(\"\\n\" + \"=\"*70)\n    print(\"âœ… PIPELINE COMPLETED SUCCESSFULLY!\")\n    print(\"=\"*70)\n    print(f\"\\nğŸ“ Generated files:\")\n    print(f\"  â€¢ best_hybrid_model.h5\")\n    print(f\"  â€¢ confusion_matrix_hybrid.png\")\n    print(f\"  â€¢ roc_curves_hybrid.png\")\n    print(f\"  â€¢ attention_visualization.png\")\n    print(f\"  â€¢ training_history.png\")\n    \n    return trained_model, history, eval_results\n\n# ---------------------------\n# RUN PIPELINE\n# ---------------------------\n\nif __name__ == \"__main__\":\n    model, history, results = main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:21:04.410612Z","iopub.execute_input":"2025-11-28T09:21:04.411193Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nHYBRID CNN-ViT FOR KNEE OSTEOARTHRITIS CLASSIFICATION\n======================================================================\n\n[STEP 1/5] Loading and preprocessing data...\nâš ï¸  This may take several minutes due to advanced preprocessing...\n\nProcessing training data...\nProcessing validation data...\nProcessing test data...\n\nâœ“ Dataset loaded:\n  Train: 5778 samples\n  Val:   826 samples\n  Test:  1656 samples\n\n[STEP 2/5] Building hybrid CNN-ViT model...\n\nâœ“ Model built successfully!\n  Total parameters: 67,431,343\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"Hybrid_CNN_ViT_KOA\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Hybrid_CNN_ViT_KOA\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer_31      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\nâ”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ \u001b[38;5;34m3\u001b[0m)                â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ patch_extractor_3   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) â”‚          \u001b[38;5;34m0\u001b[0m â”‚ input_layer_31[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mPatchExtractor\u001b[0m)    â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ patch_encoder_3     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m512\u001b[0m)  â”‚    \u001b[38;5;34m494,080\u001b[0m â”‚ patch_extractor_â€¦ â”‚\nâ”‚ (\u001b[38;5;33mPatchEncoder\u001b[0m)      â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ transformer_block_â€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m512\u001b[0m)  â”‚ \u001b[38;5;34m14,703,616\u001b[0m â”‚ patch_encoder_3[\u001b[38;5;34mâ€¦\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mTransformerBlock\u001b[0m)  â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ transformer_block_â€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m512\u001b[0m)  â”‚ \u001b[38;5;34m14,703,616\u001b[0m â”‚ transformer_blocâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mTransformerBlock\u001b[0m)  â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ transformer_block_â€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m512\u001b[0m)  â”‚ \u001b[38;5;34m14,703,616\u001b[0m â”‚ transformer_blocâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mTransformerBlock\u001b[0m)  â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ transformer_block_â€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m512\u001b[0m)  â”‚ \u001b[38;5;34m14,703,616\u001b[0m â”‚ transformer_blocâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mTransformerBlock\u001b[0m)  â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ CNN_Backbone        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m,      â”‚  \u001b[38;5;34m4,049,571\u001b[0m â”‚ input_layer_31[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mFunctional\u001b[0m)        â”‚ \u001b[38;5;34m1280\u001b[0m)             â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ layer_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m512\u001b[0m)  â”‚      \u001b[38;5;34m1,024\u001b[0m â”‚ transformer_blocâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mLayerNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ global_average_pooâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ CNN_Backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mGlobalAveragePoolâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ global_average_pooâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ layer_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mGlobalAveragePoolâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ attention_fusion_3  â”‚ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m),    â”‚  \u001b[38;5;34m2,888,706\u001b[0m â”‚ global_average_pâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mAttentionFusion\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, â”‚            â”‚ global_average_pâ€¦ â”‚\nâ”‚                     â”‚ \u001b[38;5;34m1\u001b[0m)]               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ hierarchical_classâ€¦ â”‚ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m),       â”‚  \u001b[38;5;34m1,183,498\u001b[0m â”‚ attention_fusionâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mHierarchicalClassâ€¦\u001b[0m â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, â”‚            â”‚                   â”‚\nâ”‚                     â”‚ \u001b[38;5;34m5\u001b[0m)]               â”‚            â”‚                   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer_31      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ patch_extractor_3   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ input_layer_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PatchExtractor</span>)    â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ patch_encoder_3     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">494,080</span> â”‚ patch_extractor_â€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PatchEncoder</span>)      â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ transformer_block_â€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">14,703,616</span> â”‚ patch_encoder_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)  â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ transformer_block_â€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">14,703,616</span> â”‚ transformer_blocâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)  â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ transformer_block_â€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">14,703,616</span> â”‚ transformer_blocâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)  â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ transformer_block_â€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">14,703,616</span> â”‚ transformer_blocâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)  â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ CNN_Backbone        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>,      â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> â”‚ input_layer_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)             â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ layer_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚ transformer_blocâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ global_average_pooâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ CNN_Backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePoolâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ global_average_pooâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePoolâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ attention_fusion_3  â”‚ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>),    â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,888,706</span> â”‚ global_average_pâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionFusion</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, â”‚            â”‚ global_average_pâ€¦ â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)]               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ hierarchical_classâ€¦ â”‚ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>),       â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,183,498</span> â”‚ attention_fusionâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">HierarchicalClassâ€¦</span> â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, â”‚            â”‚                   â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)]               â”‚            â”‚                   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m67,431,343\u001b[0m (257.23 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,431,343</span> (257.23 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m67,389,320\u001b[0m (257.07 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,389,320</span> (257.07 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m42,023\u001b[0m (164.16 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">42,023</span> (164.16 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n[STEP 3/5] Training model...\n\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"E0000 00:00:1764322511.083023      38 meta_optimizer.cc:966] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inHybrid_CNN_ViT_KOA_1/CNN_Backbone_1/block2b_drop_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"Batch 720/722 - Loss: 0.2560 - Acc: 0.9263\nTrain Loss: 0.2569 - Train Acc: 0.9256\nVal Loss: 4.8101 - Val Acc: 0.0473\nâœ“ Model saved!\n\nEpoch 2/100\nBatch 720/722 - Loss: 0.3257 - Acc: 0.8925\nTrain Loss: 0.3264 - Train Acc: 0.8913\nVal Loss: 21001.1406 - Val Acc: 0.1286\n\nEpoch 3/100\nBatch 720/722 - Loss: 0.3386 - Acc: 0.8696\nTrain Loss: 0.3393 - Train Acc: 0.8686\nVal Loss: 87564.0469 - Val Acc: 0.1286\n\nEpoch 4/100\nBatch 720/722 - Loss: 0.2982 - Acc: 0.8797\nTrain Loss: 0.2980 - Train Acc: 0.8798\nVal Loss: 8917.6465 - Val Acc: 0.0303\n\nEpoch 5/100\nBatch 610/722 - Loss: 0.2293 - Acc: 0.9110\r","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}}]}